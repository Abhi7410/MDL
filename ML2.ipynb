{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f132ce2e",
   "metadata": {},
   "source": [
    "# DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a03909",
   "metadata": {},
   "source": [
    "<p>\n",
    "1. Lots of data is avaliable<br>\n",
    "2. Can be in variety of forms.<br>\n",
    "3. How do u parse?<br>\n",
    "4. Is the data clean, how much noise?<br>\n",
    "5. Data for ML algorithns is divided into testing and tranining sets.<br>\n",
    "6. Training sets used to derive patterns to rebuild ML models.<br>\n",
    "7. Testing sets used to analyze performace of model.</p>\n",
    "\n",
    "<b>Signal:</b> It refers to the true underlying pattern of the data that helps the machine learning model to learn from the data.<br>\n",
    "<b>Noise:</b> Noise is unnecessary and irrelevant data that reduces the performance of the model.<br>\n",
    "<b>Bias:</b> Bias is a prediction error that is introduced in the model due to oversimplifying the machine learning algorithms. Or it is the difference between the predicted values and the actual values.<br>\n",
    "<b>Variance:</b> If the machine learning model performs well with the training dataset, but does not perform well with the test dataset, then variance occurs.<br>\n",
    "\n",
    "<h3>Goodness of Fit</h3>\n",
    "In statistics, goodness of fit refers to how closely a model’s predicted values match the observed (true) values.\n",
    "\n",
    "A model that has learned the noise instead of the signal is considered “overfit” because it fits the training dataset but has poor fit with new datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e7f89",
   "metadata": {},
   "source": [
    "<font color='red'>OVERFITTING</font>\n",
    "<p>When a model fits more data than it needs it starts catching the noisy data and inaccurate values in the data. As a result, the efficiency and accuracy of the model decrease.\n",
    "The overfitted model has low bias and high variance.</p>\n",
    "<p>The chances of occurrence of overfitting increase as much we provide training to our model. It means the more we train our model, the more chances of occurring the overfitted model.\n",
    " Overfitting is the main problem that occurs in <i>supervised learning.</i></p>\n",
    " \n",
    " <h4> How to detect Overfitting</h4>\n",
    " <p>A key challenge with overfitting, and with machine learning in general, is that we can’t know how well our model will perform on new data until we actually test it.\n",
    "\n",
    "To address this, we can split our initial dataset into separate training and test subsets.\n",
    " This method can approximate of how well our model will perform on new data.\n",
    "\n",
    "If our model does much better on the training set than on the test set, then we’re likely overfitting.\n",
    "\n",
    "For example, it would be a big red flag if our model saw 99% accuracy on the training set but only 55% accuracy on the test set.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18cda71",
   "metadata": {},
   "source": [
    "<font color='blue'>How to avoid overfitting?</font>\n",
    "<p>\n",
    "    1. <b>Cross Validation</b> : Cross-validation is a powerful preventative measure against overfitting.\n",
    "The idea is clever: Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n",
    "In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the “holdout fold”).\n",
    "</p>\n",
    "<p>\n",
    "    2. <b>Train more data</b> : It won’t work every time, but training with more data can help algorithms detect the signal better. In the earlier example of modeling height vs. age in children, it’s clear how sampling more schools will help your model.\n",
    "\n",
    "Of course, that’s not always the case. If we just add more noisy data, this technique won’t help. That’s why you should always ensure your data is clean and relevant.\n",
    "</p>\n",
    "<p>\n",
    "    3. <b>Remove Features</b>: Some algorithms have built-in feature selection.\n",
    "\n",
    "For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n",
    "\n",
    "An interesting way to do so is to tell a story about how each feature fits into the model. This is like the data scientist's spin on software engineer’s rubber duck debugging technique, where they debug their code by explaining it, line-by-line, to a rubber duck.\n",
    "\n",
    "If anything doesn't make sense, or if it’s hard to justify certain features, this is a good way to identify them.\n",
    "In addition, there are several feature selection heuristics you can use for a good starting point.\n",
    "  </p>\n",
    "  <p>\n",
    "    4. <b>Early Stopping</b>: When u are training a learning algorithm iteratively, tou can measure how well each iteration of the model performs. Up until a certual number of iterations, new iterations improve the model . After that point, however , the model's ability to generalize can weaken as it begins to overfit the training data.\n",
    "    Early stopping refers stopping the training process before the learner passes that point.\n",
    "    Today, this technique is mostly used in deep learning while other techniques (e.g. regularization) are preferred for classical machine learning.\n",
    "   </p>\n",
    "   <p>\n",
    "    5. <b>Regularization</b>: Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.\n",
    "\n",
    "The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
    "  </p>\n",
    "  <p>\n",
    "    6. <b>Ensembling</b>: Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "    Bagging attempts to reduce the chance overfitting complex models.\n",
    "\n",
    "a) It trains a large number of \"strong\" learners in parallel.<br>\n",
    "b) A strong learner is a model that's relatively unconstrained.<br>\n",
    "c) Bagging then combines all the strong learners together in order to \"smooth out\" their predictions.<br>\n",
    "Boosting attempts to improve the predictive flexibility of simple models.<br>\n",
    "\n",
    "a) It trains a large number of \"weak\" learners in sequence.<br>\n",
    "b) A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).<br>\n",
    "c) Each one in the sequence focuses on learning from the mistakes of the one before it.<br>\n",
    "d) Boosting then combines all the weak learners into a single strong learner.<br>\n",
    "    \n",
    "    Bagging uses complex base models and tries to \"smooth out\" their predictions, while boosting uses simple base models and tries to \"boost\" their aggregate complexity.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efaa42a",
   "metadata": {},
   "source": [
    "<font color='red'>UNDERFITTING</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057727be",
   "metadata": {},
   "source": [
    "<p>Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "\n",
    "In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
    "\n",
    "An underfitted model has high bias and low variance.\n",
    "\n",
    "<font color='blue'>How to avoid underfitting:</font>\n",
    "1. By increasing the training time of the model.\n",
    "2. By increasing the number of features.\n",
    "</p>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
